{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cB-S4m1ZA8oC"
   },
   "source": [
    "# Exercise 1.4\n",
    "\n",
    "## Classification of CIFAR10 images\n",
    "### Optimizers\n",
    "In this exercise we will classify the images from the CIFAR10 dataset. We will use different optimizers and compare their convergence speed. First we import the libraries that we need.\n",
    "\n",
    "**NB! The exercise is formulated in a Jupyter notebook for ease of communication, but you should feel *very* free to carry out the entire exercise without the notebook. If you do carry it out in a notebook, please finish by migrating your code over to a script that you can run from the terminal**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7z8ctltTA8oS"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Dg7mIJnA8os"
   },
   "source": [
    "We always check that we are running on a GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fLXvswP1A8os"
   },
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(\"The code will run on GPU.\")\n",
    "else:\n",
    "    print(\"The code will run on CPU. Go to Edit->Notebook Settings and choose GPU as the hardware accelerator\")\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K9FYktcoA8pB"
   },
   "source": [
    "In this exercise we will classify images from the [CIFAR10](https://www.cs.toronto.edu/~kriz/cifar.html) dataset. \n",
    "CIFAR10 has 60000 colour images of size 32x32 equally distributed in 10 classes.\n",
    "* You should load this dataset (hint: it is a built-in dataset in pytorch)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HR6Y_F1MA8pF"
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "trainset = ...\n",
    "train_loader = DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "testset = ...\n",
    "test_loader = DataLoader(testset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h7JBaV9XA8pU"
   },
   "source": [
    "* Make a CNN to train on the CIFAR10 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HAUxC2nCA8pZ"
   },
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Network, self).__init__()\n",
    "        #...\n",
    "    def forward(self, x):\n",
    "        #...\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IZNf-XPFA8pu"
   },
   "outputs": [],
   "source": [
    "model = Network()\n",
    "model.to(device)\n",
    "#Initialize the optimizer\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CqWfe3U5A8qB"
   },
   "outputs": [],
   "source": [
    "#We define the training as a function so we can easily re-use it.\n",
    "def train(model, optimizer, num_epochs=10):\n",
    "    def loss_fun(output, target):\n",
    "        return F.nll_loss(torch.log(output), target)\n",
    "    out_dict = {'train_acc': [],\n",
    "              'test_acc': [],\n",
    "              'train_loss': [],\n",
    "              'test_loss': []}\n",
    "  \n",
    "    for epoch in tqdm(range(num_epochs), unit='epoch'):\n",
    "        model.train()\n",
    "        #For each epoch\n",
    "        train_correct = 0\n",
    "        train_loss = []\n",
    "        for minibatch_no, (data, target) in tqdm(enumerate(train_loader), total=len(train_loader)):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            #Zero the gradients computed for each weight\n",
    "            optimizer.zero_grad()\n",
    "            #Forward pass your image through the network\n",
    "            output = model(data)\n",
    "            #Compute the loss\n",
    "            loss = loss_fun(output, target)\n",
    "            #Backward pass through the network\n",
    "            loss.backward()\n",
    "            #Update the weights\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss.append(loss.item())\n",
    "            #Compute how many were correctly classified\n",
    "            predicted = output.argmax(1)\n",
    "            train_correct += (target==predicted).sum().cpu().item()\n",
    "        #Comput the test accuracy\n",
    "        test_loss = []\n",
    "        test_correct = 0\n",
    "        model.eval()\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            with torch.no_grad():\n",
    "                output = model(data)\n",
    "            test_loss.append(loss_fun(output, target).cpu().item())\n",
    "            predicted = output.argmax(1)\n",
    "            test_correct += (target==predicted).sum().cpu().item()\n",
    "        out_dict['train_acc'].append(train_correct/len(trainset))\n",
    "        out_dict['test_acc'].append(test_correct/len(testset))\n",
    "        out_dict['train_loss'].append(np.mean(train_loss))\n",
    "        out_dict['test_loss'].append(np.mean(test_loss))\n",
    "        print(f\"Loss train: {np.mean(train_loss):.3f}\\t test: {np.mean(test_loss):.3f}\\t\",\n",
    "              f\"Accuracy train: {out_dict['train_acc'][-1]*100:.1f}%\\t test: {out_dict['test_acc'][-1]*100:.1f}%\")\n",
    "    return out_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HB7NHbYfA8qf"
   },
   "source": [
    " * Train the network and plot make a plot of the loss and accuracy for both training and with the epoch on the x-axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c053536qA8qk"
   },
   "outputs": [],
   "source": [
    "out_dict = train(model, optimizer)\n",
    "# ...\n",
    "# ...\n",
    "plt.legend(('Test error','Train eror'))\n",
    "plt.xlabel('Epoch number')\n",
    "plt.ylabel('Accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bmxbypURA8q2"
   },
   "source": [
    "* Discuss what you see. Are you overfitting to the training data? Do you not learn anything? What can you change to do better?\n",
    "\n",
    "* Repeat the above steps but using Adam as the optimizer. Use Pytorch's defaults parameters. Do you learn faster?\n",
    "* Which optimizer works best for you?\n",
    "* Plot the test and test errors for both SGD and Adam in one plot\n",
    "* Try adding Batch normalisation after your convolutional layers. Does it help?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xMMyz4WhA8q6"
   },
   "source": [
    "## ResNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QQhB2BH3A8q8"
   },
   "source": [
    "Now you will create and train a ResNet.\n",
    "* Implement the Residual block as a network below using convolutional kernel size $3\\times3$ according to the figure below\n",
    "![Residual block](https://cdn-images-1.medium.com/max/800/1*D0F3UitQ2l5Q0Ak-tjEdJg.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g-FoRHmrA8rC"
   },
   "outputs": [],
   "source": [
    "class ResNetBlock(nn.Module):\n",
    "    def __init__(self, n_features):\n",
    "        super(ResNetBlock, self).__init__()\n",
    "        ...\n",
    "    def forward(self, x):\n",
    "        ...\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WZDp_0SgA8rS"
   },
   "source": [
    "The following code is a sanity of your residual block network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9YRnqMaTA8rW"
   },
   "outputs": [],
   "source": [
    "#Sanity test of your implementation\n",
    "C = 4\n",
    "res_block = ResNetBlock(C)\n",
    "assert(len(res_block.state_dict())==4)\n",
    "for name, weight in res_block.state_dict().items():\n",
    "    weight*=0\n",
    "    desired_shape = {'bias': (C,), 'weight': (C, C, 3, 3)}[name.split('.')[-1]]\n",
    "    assert(desired_shape==weight.shape)\n",
    "x = torch.randn(32, C, 32,32)\n",
    "assert(torch.abs(res_block(x)-F.relu(x)).max()==0)\n",
    "print(\"Passed sanity check\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zKFu5rWDA8rv"
   },
   "source": [
    "We define a network that uses your `ResNetBlock`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "waKiIu5NA8r2"
   },
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(self, n_in, n_features, num_res_blocks=3):\n",
    "        super(ResNet, self).__init__()\n",
    "        #First conv layers needs to output the desired number of features.\n",
    "        conv_layers = [nn.Conv2d(n_in, n_features, kernel_size=3, stride=1, padding=1),\n",
    "                       nn.ReLU()]\n",
    "        for i in range(num_res_blocks):\n",
    "            conv_layers.append(ResNetBlock(n_features))\n",
    "        self.res_blocks = nn.Sequential(*conv_layers)\n",
    "        self.fc = nn.Sequential(nn.Linear(32*32*n_features, 2048),\n",
    "                                nn.ReLU(),\n",
    "                                nn.Linear(2048, 512),\n",
    "                                nn.ReLU(),\n",
    "                                nn.Linear(512,10),\n",
    "                                nn.Softmax(dim=1))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.res_blocks(x)\n",
    "        #reshape x so it becomes flat, except for the first dimension (which is the minibatch)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        out = self.fc(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QF1XvsLGA8sH"
   },
   "source": [
    "Let's train our new ResNet!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qWychvPvA8sN"
   },
   "outputs": [],
   "source": [
    "model = ResNet(3, 8)\n",
    "model.to(device)\n",
    "#Initialize the optimizer\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "out_dict = train(model, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KoLIRQCr23Es"
   },
   "source": [
    "\n",
    "\n",
    "Do you get nan loss at some point during training? \n",
    "This can be caused by the numerical instability of using softmax and log as two functions. \n",
    "* Change your network and loss to use a layer that combines the softmax log into one such as `nn.LogSoftmax`. You can also use `nn.CrossEntropyLoss` which also integrates `nn.NLLLoss`."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Exercise 1.3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
